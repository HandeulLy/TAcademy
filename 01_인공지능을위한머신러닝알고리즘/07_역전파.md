

# ﻿﻿[Tacademy] 인공지능을 위한 머신러닝 알고리즘 - 07강 역전파



## **# 역전파 학습 방법**

**※ 가중치 in 단층 퍼셉트론**

  · 단층 퍼셉트론 모델에서는 가중치를 찾기 위해 경사 하강법(Gradient Descent)을 사용, 가중치 값은 입력과 오류(target과 모델 예측 값의 차이)의 크기에 비례



**※ 가중치 in 다층 퍼셉트론**

  · 다층 퍼셉트론에서는 출력층에 인접한 마지막 은닉층에 의한 에러 신호만 존재하고, 입력층에서의 직접적인 에러는 없음



**※ 기여도 할당 문제**

  · 다층 퍼셉트론에서 각 층마다의 가중치 계산을 위해 고안된 것

  · 전체 학습 모델을 구성하는데에 관여하고 있는 모든 개별 요소들에게 '기여도' 또는 '책임'을 할당하는 문제

  · 다층 신경망에서 어떤 가중치들을 얼만큼, 어떤 방향으로 학습시키는지에 대한 것

  · 앞 부분 레이어의 가중치들을 최종 출력에 얼만큼 영향을 줄지 결정하는 문제



**※ 역전파(Backpropagation)**

  · 다층 퍼셉트론에서 기여도 할당 문제에 대한 해결책

  · 1단계 : 입력 값들을 사용하여 다층 퍼셉트론의 은닉층과 출력층의 값을 계산

  · 2단계 : 손실함수 식을 따라 에러 값을 계산한 뒤, 최종 출력 유닛들로부터 시작하여 네트워크의 후방으로 에러 값을 전파(은닉층과 출력층 모두에 적용), 에러 값을 줄이기 위해 경사 하강법을 사용하여 가중치를 변경하는 것



## **# 활성함수의 미분값**

**※ 활성함수에 대한 에러의 변화량**

  · 활성함수에 대한 미분을 계산해야 함

  · 가중치의 변화량은 활성함수 미분값에 비례

  · 대표 활성화 함수인 시그모이드는 값이 너무 크거나 작을 때 미분값이 0에 가까워지기 때문에... 깊은 신경망 학습이 잘 진행되지 않음