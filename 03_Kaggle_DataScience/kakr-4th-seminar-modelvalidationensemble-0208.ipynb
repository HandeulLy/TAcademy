{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Understanding Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before Start\n본격적으로 시작하기 전에 데이터에 대해서 아주 약간만 깊이 있게 이해해보는 시간을 가져보려고 합니다."},{"metadata":{},"cell_type":"markdown","source":"### Q. 디스크의 CSV파일의 용량은 그렇게 높진 않은데 메모리로 읽기만 하면 몇 배로 늘어나는 이유는?\n캐글을 하시다 보면 이런 경험이 한번쯤은 다들 있으실 것 같습니다.   \n분명히 CSV 파일로는 1GB 보다 아래였는데 판다스로 read를 하면 2~3GB로 늘어나는 경우가 종종 있는데, 이유가 무엇일까요?"},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"PATH = '../input/kakr-4th-competition/'\ntrain = pd.read_csv(PATH + 'train.csv')\ntest  = pd.read_csv(PATH + 'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 데이터 확인"},{"metadata":{},"cell_type":"markdown","source":"* id\n* age : 나이\n* workclass : 고용 형태\n* fnlwgt : 사람 대표성을 나타내는 가중치 (final weight의 약자)\n* education : 교육 수준\n* education_num : 교육 수준 수치\n* marital_status: 결혼 상태\n* occupation : 업종\n* relationship : 가족 관계\n* race : 인종\n* sex : 성별\n* capital_gain : 양도 소득\n* capital_loss : 양도 손실\n* hours_per_week : 주당 근무 시간\n* native_country : 국적\n* income : 수익 (예측해야 하는 값)\n    * \\>50K : 1\n    * <=50K : 0"},{"metadata":{},"cell_type":"markdown","source":"# 데이터 확인"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target에 해당하는 컬럼을 바로 label로 지정하고, train_data_set에서는 제외하는 것도 방법\nlabel = train['income']\n\ndel train['income']\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lable에 대해 범위를 지정해서 값 변경, 대회 규칙에 따라서\nlabel = label.map(lambda x : 1 if x== '>50K' else 0) # True/False 값을 int로 변형해서 저장\n\nlabel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# id 컬럼은 필요 없는 컬럼이기 때문에 삭제\n\ndel train['id']\ndel test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 컬럼들에 대한 정보 확인\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 컬럼들 중 수치 값을 갖는 컬럼에 대한 통계적 수치 정보 확인\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 결측치 처리\n\n* workclass, occupation, native_country 컬럼에 결측치가 존재\n* '?'로 표시된 데이터는 해당 컬럼의 최빈값으로 결측치를 처리\n\n\n* 일반적으로는 최빈값을 통해 대체하는 경우가 많지만,\n* 만약 다른 컬럼을 통해서 처리할 수 있다면 그렇게 하는 것이 더 좋은 방법\n* ex) education_num 등\n\n\n* 정답은 없고, 해당 데이터의 상태를 보고서 결정해야 함"},{"metadata":{"trusted":true},"cell_type":"code","source":"has_na_colums = ['workclass', 'occupation', 'native_country']\n\nfor c in has_na_colums :\n    # mode() 해당 컬럼의 최빈 값을 가져오는 함수\n    train.loc[train[c] == '?', c]  = train[c].mode()[0]\n    test.loc[test[c] == '?', c]  = test[c].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 적용 후 결측치 처리가 된건지 확인\n(train[has_na_colums] == '?').sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Log 변환"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['capital_gain'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 치우친 정도를 완화시키기 위해 log 변환 적용\n\ntrain['log_capital_gain'] = train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\ntest['log_capital_gain'] = test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 적용 후 확인\n\ntrain['log_capital_gain'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이제 원래 컬럼은 필요 없다고 판단하고 삭제\n\ntrain = train.drop(columns=['capital_gain'])\ntest = test.drop(columns=['capital_gain'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# capital_loss 컬럼에도 적용\n\ntrain['log_capital_loss'] = train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\ntest['log_capital_loss'] = test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n\ntrain = train.drop(columns=['capital_loss'])\ntest = test.drop(columns=['capital_loss'])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 데이터 쪼개기, Split"},{"metadata":{},"cell_type":"markdown","source":"* Train Data : 모델을 학습하는 데 사용하는 데이터(모델이 알고 있는 학습할 데이터, 과거 데이터)\n* Valid Data : 학습한 모델의 성능을 검증하는 데이터(모델이 모르는 학습하지 않을 데이터, 과거 데이터)\n* Test Data : 학습한 모델로 예측할 데이터(모델이 모르는 데이터, 미래 데이터)"},{"metadata":{},"cell_type":"markdown","source":"* train_test_split\n    * test_size : Valid(test)의 크기 비율 지정\n    * random_state : 데이터 쪼갤 때 내부적으로 사용하는 난수 값(default는 매번 다름)\n    * shuffle : 데이터 쪼갤 때 섞을지 유무 지정\n    * stratify : 쪼개기 이전의 클래스 비율을 쪼개고 난 후에도 유지하기 위해 설정하는 값"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(\n    train, label,\n    test_size = 0.3,\n    random_state = 2021,\n    shuffle = True,\n    stratify = label\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 인덱스 초기화\nx_train = x_train.reset_index(drop=True)\nx_valid = x_valid.reset_index(drop=True)\n\ntest = test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 스케일링, Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scikit-learn 라이브러리에 있는 Standard Scaler 사용해서 수치형 변수들의 표준화 진행\n\nctgy_col = [c for c, t in zip(x_train.dtypes.index, x_train.dtypes) if t == 'O'] # categorical columns\nnumr_col = [c for c in x_train.columns if c not in ctgy_col] # numerical columns\n\nprint(ctgy_col)\nprint(numr_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling과 같은 전처리는 항상 train data를 기준으로 진행\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train[numr_col] = scaler.fit_transform(x_train[numr_col])\n\n# 여기서 valid와 test는 fit하면 안됨\nx_valid[numr_col] = scaler.transform(x_valid[numr_col])\ntest[numr_col] = scaler.transform(test[numr_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 인코딩, Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 범주형 변수는 Onehot Encoding 적용\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ntmp_all = pd.concat([x_train, x_valid, test])\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(tmp_all[ctgy_col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe_col = list()\n\nfor lst in ohe.categories_ :\n    ohe_col += lst.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_ctgy = pd.DataFrame(ohe.transform(x_train[ctgy_col]), columns=ohe_col)\nnew_valid_ctgy = pd.DataFrame(ohe.transform(x_valid[ctgy_col]), columns=ohe_col)\nnew_test_ctgy = pd.DataFrame(ohe.transform(test[ctgy_col]), columns=ohe_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_ctgy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 인코딩 적용\nx_train = pd.concat([x_train, new_train_ctgy], axis=1)\nx_valid = pd.concat([x_valid, new_valid_ctgy], axis=1)\ntest = pd.concat([test, new_test_ctgy], axis=1)\n\n# 기존 컬럼 제거\nx_train = x_train.drop(columns = ctgy_col)\nx_valid = x_valid.drop(columns = ctgy_col)\ntest = test.drop(columns = ctgy_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scikit-learn 기반 분류 모델"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 로지스틱 회귀 모델\n\nlr = LogisticRegression()\n\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_valid)\n\nf1_score(y_valid, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 서포트 벡터 머신(rbf 커널)\n\nsvm = SVC()\n\nsvm.fit(x_train, y_train)\n\ny_pred = svm.predict(x_valid)\n\nf1_score(y_valid, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 랜덤 포레스트\n\nrf = RandomForestClassifier()\n\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_valid)\n\nf1_score(y_valid, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGB\n\nxgb = XGBClassifier()\n\nxgb.fit(x_train, y_train)\n\ny_pred = xgb.predict(x_valid)\n\nf1_score(y_valid, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBM\n\nlgbm = LGBMClassifier()\n\nlgbm.fit(x_train, y_train)\n\ny_pred = lgbm.predict(x_valid)\n\nf1_score(y_valid, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. k-Fold Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess 진행한 data 확인\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/kakr-4th-competition/train.csv\")\nlabel = train['income']\n\ndel train['income']\n\ntest = pd.read_csv(\"/kaggle/input/kakr-4th-competition/test.csv\")\n\nhas_na_columns = ['workclass', 'occupation', 'native_country']\n\nfrom sklearn.preprocessing import StandardScaler\n\ncat_columns = [c for c, t in zip(train.dtypes.index, train.dtypes) if t == 'O'] \nnum_columns = [c for c in train.dtypes.index if c not in cat_columns]\n\nprint('범주형 변수: \\n{}\\n\\n 수치형 변수: \\n{}\\n'.format(cat_columns, num_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x_train, x_valid, x_test):\n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    for c in has_na_columns:\n        tmp_x_train.loc[tmp_x_train[c] == '?', c] = tmp_x_train[c].mode()[0]\n        tmp_x_valid.loc[tmp_x_valid[c] == '?', c] = tmp_x_valid[c].mode()[0]\n        tmp_x_test.loc[tmp_x_test[c]   == '?', c] = tmp_x_test[c].mode()[0]\n    \n    tmp_x_train['log_capital_loss'] = tmp_x_train['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_loss'] = tmp_x_valid['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_loss'] = tmp_x_test['capital_loss'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train['log_capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_valid['log_capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    tmp_x_test['log_capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_train = tmp_x_train.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_valid = tmp_x_valid.drop(columns=['capital_loss', 'capital_gain'])\n    tmp_x_test  = tmp_x_test.drop(columns=['capital_loss', 'capital_gain'])\n    \n    scaler = StandardScaler()\n    tmp_x_train[num_columns] = scaler.fit_transform(tmp_x_train[num_columns])\n    tmp_x_valid[num_columns] = scaler.transform(tmp_x_valid[num_columns])\n    tmp_x_test[num_columns]  = scaler.transform(tmp_x_test[num_columns])\n    \n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n\n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(tmp_all[cat_columns])\n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nn_splits = 5\n\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = 2021)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_f1(y, t, treshold=0.5) : \n    t = t.get_label()\n    y_bin = (y > threshold).astype(int)\n    return 'f1', f1_score(t, y_bin, averager='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_scores = list()\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)) : \n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # 전처리\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # 모델 정의\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # 모델 학습\n    clf.fit(\n        x_train, y_train,\n        eval_set = [[x_valid, y_valid]],\n        eval_metric = xgb_f1,\n        early_stopping_rounds = 100,\n        verbose = 100\n    )\n    \n    # 훈련, 검증 데이터 Log Loss 확인\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}, validation f1_score : {:.4f}\\n'. format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n# 교차 검증 F1 Score 평균 계산\nprint('Cross Validation Score : {:.4f}'. format(np.mean(val_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OOF"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_scores = list()\noof_pred = np.zeros((test.shape[0], )) #\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # 전처리\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # 모델 정의\n    clf = XGBClassifier(tree_method='gpu_hist')\n    \n    # 모델 학습\n    clf.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = xgb_f1,        \n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # 훈련, 검증 데이터 F1 Score 확인\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[: , 1] / n_splits #\n    \n\n# 교차 검증 F1 Score 평균 계산하기\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STACKING"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}